{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "73d70767",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn\n",
    "import sys\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import ldamodel\n",
    "import gensim.corpora\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import normalize\n",
    "import pickle\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "import pyLDAvis.gensim\n",
    "import warnings\n",
    "from itertools import chain\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lmtzr = WordNetLemmatizer()\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "from tqdm._tqdm_notebook import tqdm_notebook,tnrange,tqdm\n",
    "from collections import Counter,OrderedDict\n",
    "from gensim import models,corpora\n",
    "import warnings\n",
    "import pyLDAvis.gensim\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pyLDAvis.gensim\n",
    "import gensim.models.phrases as gen\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import spacy\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import spacy\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# libraries for visualization\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d3bbdb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0            Paper_ID Question_ID Question_Type  \\\n",
      "0           0  0460_m17_qp_22.pdf           1          MAIN   \n",
      "1           1  0460_m17_qp_22.pdf         (a)           SUB   \n",
      "\n",
      "                                       Question_Text  \\\n",
      "0  Study the map extract for Ballyvaghan, Ireland...   \n",
      "1  The map has blue grid lines which make squares...   \n",
      "\n",
      "                                          Token_Text  Word_Count  \n",
      "0                                  map extract scale          71  \n",
      "1  map lines squares area land grid square show a...         126  \n",
      "2655\n",
      "Question_Text\n",
      "2476\n",
      "Token_Text\n",
      "2020\n"
     ]
    }
   ],
   "source": [
    "review_data= pd.read_csv(\"scrape.csv\")\n",
    "print(review_data.head(2))\n",
    "print(len(review_data))\n",
    "print('Question_Text')\n",
    "print(len(review_data.groupby('Question_Text')))\n",
    "print('Token_Text')\n",
    "print(len(review_data.groupby('Token_Text')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66710186",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text ): \n",
    "    delete_dict = {sp_character: '' for sp_character in string.punctuation} \n",
    "    delete_dict[' '] = ' ' \n",
    "    table = str.maketrans(delete_dict)\n",
    "    text1 = text.translate(table)\n",
    "    textArr= text1.split()\n",
    "    text2 = ' '.join([w for w in textArr if ( not w.isdigit() and  ( not w.isdigit() and len(w)>3))]) \n",
    "    \n",
    "    return text2.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d035be6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/chae/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords') # run this one time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea3e64d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions with words within given range:\n",
      "301\n"
     ]
    }
   ],
   "source": [
    "# To find text length boundaries\n",
    "review_data.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "review_data['Question_Text'] = review_data['Question_Text'].apply(clean_text)\n",
    "review_data['Num_words_text'] = review_data['Question_Text'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "mask = (review_data['Num_words_text'] < 100) & (review_data['Num_words_text'] >= 20)\n",
    "df_within = review_data[mask]\n",
    "\n",
    "df_sampled = df_within.groupby('Paper_ID').apply(lambda x: x.sample(n=min(20000, len(x)))).reset_index(drop=True)\n",
    "\n",
    "print('Number of questions with words within given range:')\n",
    "print(len(df_within))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "089e78ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions containing \"km\":\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with missing values\n",
    "review_data.dropna(axis=0, how='any', inplace=True)\n",
    "pattern = r'km\\d*'\n",
    "# Clean text and calculate the number of words in each question\n",
    "review_data['Question_Text'] = review_data['Question_Text'].apply(clean_text)\n",
    "store = review_data['Question_Text']\n",
    "# review_data['Contains_km'] = review_data['Question_Text'].apply(lambda x: 'km' in str(x).lower())\n",
    "review_data['Contains_km'] = review_data['Question_Text'].apply(lambda x: bool(re.search(pattern, str(x).lower())))\n",
    "\n",
    "# Filter questions containing 'km'\n",
    "df_containing_km = review_data[review_data['Contains_km']]\n",
    "\n",
    "print('Number of questions containing \"km\":')\n",
    "print(len(df_containing_km))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2882e931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words count : 1\n"
     ]
    }
   ],
   "source": [
    "test_str = 'hi i am chae'\n",
    "\n",
    "letter = \"am\"\n",
    "\n",
    "res = len([ele for ele in test_str.split() if letter in ele])\n",
    "\n",
    "print(\"Words count : \" + str(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a0e3e1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 0.1\n",
    "BETA = 0.1\n",
    "NUM_TOPICS = 20\n",
    "sp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "718e875a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_frequencies(review_data, max_docs=10000):\n",
    "    freqs = Counter()\n",
    "    all_stopwords = sp.Defaults.stop_words\n",
    "    all_stopwords.add('enron')\n",
    "    nr_tokens = 0\n",
    "\n",
    "    for doc in review_data[:max_docs]:\n",
    "        tokens = sp.tokenizer(doc)\n",
    "        for token in tokens:\n",
    "            token_text = token.text.lower()\n",
    "            if token_text not in all_stopwords and token.is_alpha:\n",
    "                nr_token += 1\n",
    "                freqs[token_text] += 1\n",
    "    return\n",
    "\n",
    "def get_vocab(freqs, freq_threshold=3):\n",
    "    vocab = {}\n",
    "    vocab_idx_str = {}\n",
    "    vocab_idx = 0\n",
    "\n",
    "    for word in freqs:\n",
    "        if freqs[words] >= freq_threshold:\n",
    "            vocab[word] = vocab_idc\n",
    "            vocab_idx_str[vocab_idx] = word\n",
    "\n",
    "    return vocab, vocab_idx_str\n",
    "\n",
    "def tokenize_dataset(review_data, vocab, max_docs=10000):\n",
    "    nr_tokens = 0\n",
    "    nr_docs = 0\n",
    "    docs = []\n",
    "\n",
    "    for doc in review_data[:max_docs]:\n",
    "        tokens = sp.tokenizer(doc)\n",
    "\n",
    "        if len(tokens) > 1:\n",
    "            doc = []\n",
    "            for token in tokens:\n",
    "                token_text = token.text.lower()\n",
    "                if token_text in vocab:\n",
    "                    doc.append(token_text)\n",
    "                    nr_tokens += 1\n",
    "            nr_docs += 1\n",
    "            docs.append(doc)\n",
    "\n",
    "    print(f'Number of questions: {nr_docs}')\n",
    "    print(f'Number of words: {nr_tokens}')\n",
    "\n",
    "    corpus = []\n",
    "    for doc in docs:\n",
    "        corpus_d = []\n",
    "\n",
    "        for token in doc:\n",
    "            corpus_d.append(vocab[token])\n",
    "\n",
    "        corpus.append(np.asarray(corpus_d))\n",
    "\n",
    "    return docs, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "513508ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'nr_token' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m data \u001b[38;5;241m=\u001b[39m review_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuestion_Text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msample(frac\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m----> 2\u001b[0m freqs \u001b[38;5;241m=\u001b[39m generate_frequencies(data, max_docs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m)\n\u001b[1;32m      3\u001b[0m vocab, vocab_idx_str \u001b[38;5;241m=\u001b[39m get_vocab(freqs)\n\u001b[1;32m      4\u001b[0m docs, corpus \u001b[38;5;241m=\u001b[39m tokenize_dataset(data, vocab)\n",
      "Cell \u001b[0;32mIn[68], line 12\u001b[0m, in \u001b[0;36mgenerate_frequencies\u001b[0;34m(review_data, max_docs)\u001b[0m\n\u001b[1;32m     10\u001b[0m         token_text \u001b[38;5;241m=\u001b[39m token\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m token_text \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m all_stopwords \u001b[38;5;129;01mand\u001b[39;00m token\u001b[38;5;241m.\u001b[39mis_alpha:\n\u001b[0;32m---> 12\u001b[0m             nr_token \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     13\u001b[0m             freqs[token_text] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'nr_token' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "data = review_data[\"Question_Text\"].sample(frac=0.5, random_state=42).values\n",
    "freqs = generate_frequencies(data, max_docs=10000)\n",
    "vocab, vocab_idx_str = get_vocab(freqs)\n",
    "docs, corpus = tokenize_dataset(data, vocab)\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2fa56d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions containing \"km\" or \"km2\", etc.:\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Define a regex pattern to match variations of \"km\"\n",
    "pattern = r'km\\d*'  # This matches \"km\" followed by zero or more digits (\\d*)\n",
    "\n",
    "# Apply the regex pattern to each question text\n",
    "review_data['Contains_km'] = review_data['Question_Text'].apply(lambda x: bool(re.search(pattern, str(x).lower())))\n",
    "\n",
    "# Filter questions containing the pattern\n",
    "df_containing_km = review_data[review_data['Contains_km']]\n",
    "\n",
    "print('Number of questions containing \"km\" or \"km2\", etc.:')\n",
    "print(len(df_containing_km))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2c6a924b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'toward', 'various', 'her', 'i', 'towards', \"n't\", 'such', 'so', 'whereas', 'no', 'across', 'the', 'thence', 'has', 'call', 'unless', 'how', 'in', 'via', \"'ll\", 'eleven', 'two', 'more', 'keep', 'get', \"'s\", 'if', 'yourselves', 'n‘t', 'other', 'whether', \"'m\", \"'re\", 'been', 'anywhere', 'those', 'onto', 'please', 'all', 'our', 'some', 'well', 'five', 'us', 'therein', 'at', 'nevertheless', '‘re', 'under', 'besides', 'many', 'almost', 'over', 'during', 'fifty', 'their', 'neither', 'thereupon', 'by', 'above', 'three', 'could', 'thereafter', 'since', '’ll', 'never', 'ourselves', 'either', 'an', 'was', 'too', 'something', 'put', 'are', 'for', 'therefore', 'also', 'however', 'both', 'perhaps', 'wherever', 'it', 'themselves', 'himself', 'its', 'go', 'up', 'even', 'my', 'sometimes', 'we', 're', 'than', 'you', '’ve', 'must', 'rather', 'top', 'most', 'few', 'along', 'show', 'wherein', '‘m', 'though', \"'ve\", 'off', 'fifteen', 'yours', 'now', 'much', 'hers', 'enough', 'alone', 'otherwise', 'somewhere', 'within', 'same', 'amongst', 'latterly', 'someone', 'herself', 'take', 'everything', 'others', 'without', 'amount', 'whereupon', 'ever', 'have', 'still', 'third', 'not', 'several', 'really', 'whereby', 'would', 'twelve', '‘ll', 'last', 'upon', 'becomes', 'them', 'together', 'below', 'serious', 'already', 'or', 'had', 'am', 'out', 'behind', 'side', 'him', 'moreover', 'that', 'whole', '’re', 'will', 'quite', 'using', 'whither', 'least', '’m', 'hereupon', 'make', 'full', 'were', 'may', 'these', 'namely', 'thru', 'name', 'else', 'mostly', 'throughout', 'she', 'whoever', 'afterwards', 'indeed', 'to', 'ten', 'with', 'should', '‘d', 'because', 'per', 'nor', '‘s', 'beyond', 'four', 'before', 'whence', 'which', 'a', 'do', 'hereafter', 'then', 'they', 'be', 'there', 'former', 'on', 'what', 'less', 'whereafter', 'see', 'while', 'down', 'nobody', 'move', 'regarding', 'against', 'but', 'and', 'might', 'seems', 'become', 'first', 'just', 'after', 'hundred', 'nothing', '’d', 'part', 'twenty', 'anyhow', 'where', 'any', 'eight', 'sometime', 'he', 'once', 'next', 'again', 'nowhere', 'thereby', 'hence', 'until', 'his', 'ours', 'elsewhere', 'n’t', 'thus', 'anyway', 'although', 'ca', 'whenever', 'back', 'say', 'your', 'except', 'mine', 'why', 'latter', 'cannot', 'bottom', 'itself', 'does', 'forty', 'used', 'front', 'enron', 'of', 'seemed', 'none', 'hereby', 'this', 'whatever', 'seem', 'often', 'everyone', 'myself', 'own', 'beforehand', 'as', 'everywhere', 'me', 'from', 'done', 'through', 'sixty', '‘ve', 'empty', 'anything', 'who', '’s', 'about', 'being', 'due', 'another', 'made', 'every', 'always', 'give', 'one', 'very', 'somehow', 'only', 'herein', 'whose', 'further', 'can', 'whom', 'among', 'meanwhile', 'around', 'nine', 'yourself', 'when', 'anyone', 'here', \"'d\", 'beside', 'seeming', 'between', 'doing', 'became', 'did', 'six', 'becoming', 'formerly', 'into', 'is', 'yet', 'each', 'noone'}\n"
     ]
    }
   ],
   "source": [
    "print(sp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc99713e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
